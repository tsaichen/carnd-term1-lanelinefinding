{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Project: **Finding Lane Lines on the Road** \n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "Pipeline for this project consist of the following steps:\n",
    "- Converting image to gray scale after color selection to help capture lane lines\n",
    "- Use Canny edge detection to detect edges in the images\n",
    "- Use Hough Transform to convert detected edges to lines in the region of interest in the image\n",
    "- Use least square fit to find the linear line which are the lane lines in the images \n",
    "- Combine the lane lines found with the original image and output final image\n",
    "- Process video clips as images and apply the above steps to add lane lines to output clip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages used for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import cm\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages and function used for os to create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"test_images/\")\n",
    "\n",
    "def create_dir(dir_name):\n",
    "    try:\n",
    "        os.stat(dir_name)\n",
    "    except:\n",
    "        os.mkdir(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Processing**\n",
    "\n",
    "Below are the two main types of lane lines detected in the project, one type is white lane lines, the other is yellow lane lines. \n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images/solidWhiteCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    " <img src=\"test_images/solidYellowCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "After some iterations based on different images with different brightness and shading (especially after the challenge.mp4 video), the approach chosen was to first convert the image to HLS color scale and filter only white and yellow from the images. Below plots show the result after converting the images to gray scale, which we can clearly detect the lane lines. \n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_gray/solidWhiteCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    " <img src=\"test_images_gray/solidYellowCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "Next step was to apply Canny edge detection (by using cv2.Canny) to capture edges. Different Gaussian filter kernel size was tested to smooth out the unwanted objects in the plot. Final kernel size used in the project was 13. \n",
    "\n",
    "Below plot shows the region of interest in the images used since the rest of the images will not contain the lane lines.\n",
    "\n",
    "<figure>\n",
    " <img src=\"examples/region1.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "After Hough transform (by using cv2.HoughLinesP), lane lines were found as shown in plots below. \n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output_no_averaging/solidWhiteCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    " <img src=\"test_images_output_no_averaging/solidYellowCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "Next step was to take the lines from Hough transform and use least square fit to find the optimal line to represent the lane lines. The idea was to get the left and right lane lines by distinghish the lane lines based on positive (right) and negative (left) slopes. However, as discovered while checking the result from challenge.mp4 video, there could be \n",
    "lines with different slopes on the opposite of the plot which will cause some jumpy lines in the video. To resolve this issue, a boundary condition was set to ensure that the right and left lane lines stay on proper side of the image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/solidWhiteCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    " <img src=\"test_images_output/solidYellowCurve.jpg\" width=\"320\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def rgb2hls (img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "def white_yellow_hls(img): \n",
    "    # get white and yellow colors in image only\n",
    "    # white color mask\n",
    "    \n",
    "    white_mask_lower = np.uint8([  0, 200,   0])\n",
    "    white_mask_upper = np.uint8([179, 255, 255])\n",
    "    white_mask = cv2.inRange(img, white_mask_lower, white_mask_upper)\n",
    "    # yellow color mask\n",
    "    yellow_mask_lower = np.uint8([ 15,   0, 80])\n",
    "    yellow_mask_upper = np.uint8([ 45, 255, 255])\n",
    "    yellow_mask = cv2.inRange(img, yellow_mask_lower, yellow_mask_upper)\n",
    "\n",
    "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
    "    return cv2.bitwise_and(img, img, mask = mask)\n",
    "    \n",
    "\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=10):\n",
    "    \"\"\"\n",
    "    NOTE: this is the function you might want to use as a starting point once you want to \n",
    "    average/extrapolate the line segments you detect to map out the full\n",
    "    extent of the lane (going from the result shown in raw-lines-example.mp4\n",
    "    to that shown in P1_example.mp4).  \n",
    "    \n",
    "    Think about things like separating line segments by their \n",
    "    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n",
    "    line vs. the right line.  Then, you can average the position of each of \n",
    "    the lines and extrapolate to the top and bottom of the lane.\n",
    "    \n",
    "    This function draws `lines` with `color` and `thickness`.    \n",
    "    Lines are drawn on the image inplace (mutates the image).\n",
    "    If you want to make the lines semi-transparent, think about combining\n",
    "    this function with the weighted_img() function below\n",
    "    \"\"\"\n",
    "    \n",
    "    if lines is not None: # check point for images that do not have lines after hough transform        \n",
    "        left_slope_arr = []\n",
    "        left_slope_x = []\n",
    "        left_slope_y = []\n",
    "        right_slope_arr = []\n",
    "        right_slope_x = []\n",
    "        right_slope_y = []\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                if (x2 - x1) == 0: # take out all vertical lines since slope --> inf\n",
    "                    continue \n",
    "                slope = (y2 - y1) / (x2 - x1)\n",
    "                if slope > 0 and (min(x1,x2) > (img.shape[1]>>1)): # x,y is backwards in plot domain, right slope is positive.\n",
    "                    right_slope_arr.append(slope)\n",
    "                    right_slope_x.extend([x1, x2])\n",
    "                    right_slope_y.extend([y1, y2])\n",
    "                elif slope < 0  and (max(x1,x2) < (img.shape[1]>>1)) :\n",
    "                    left_slope_arr.append(slope)\n",
    "                    left_slope_x.extend([x1, x2])\n",
    "                    left_slope_y.extend([y1, y2])\n",
    "        #print (right_slope_x, len(right_slope_x))\n",
    "        \n",
    "        \n",
    "        right_slope_a, right_slope_b = np.polyfit(right_slope_x, right_slope_y,1)\n",
    "        left_slope_a, left_slope_b = np.polyfit(left_slope_x, left_slope_y,1)\n",
    "        \n",
    "        \n",
    "        #extrapolating \n",
    "        #x_min = min(right_slope_x_min, left_slope_x_min)\n",
    "        #draw right slope line\n",
    "        # x = (y-b)/a, set line boundaries to be the low side of the plot to close to the middle of the plot\n",
    "        right_slope_x1 = int((img.shape[0] - right_slope_b ) / right_slope_a)\n",
    "        right_slope_x2 = int(((img.shape[0] * 0.6 ) - right_slope_b ) / right_slope_a)\n",
    "        cv2.line(img, (right_slope_x1, img.shape[0]), (right_slope_x2, int(img.shape[0] * 0.6)), color, thickness)\n",
    "        #draw left slope line\n",
    "        # x = (y-b)/a\n",
    "        left_slope_x1 = int((img.shape[0] - left_slope_b ) / left_slope_a)\n",
    "        left_slope_x2 = int(((img.shape[0] * 0.6 ) - left_slope_b ) / left_slope_a)\n",
    "        cv2.line(img, (left_slope_x1, img.shape[0]), (left_slope_x2, int(img.shape[0] * 0.6)), color, thickness)\n",
    "\n",
    "def draw_lines_no_averaging(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img, lines)\n",
    "    return line_img\n",
    "\n",
    "# Python 3 has support for cool math symbols.\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1.0, λ=0.0):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse each file in test image folder\n",
    "for img in os.listdir(\"test_images/\"): \n",
    "    image = mpimg.imread(\"test_images/\"+img)\n",
    "    line_image = np.copy(image)*1 # creating a blank to draw lines on\n",
    "    test = np.copy(image)*1 # creating a blank to draw lines on\n",
    "    image = rgb2hls(image)\n",
    "    image = white_yellow_hls(image)\n",
    "    \n",
    "    create_dir(\"test_images_gray\")\n",
    "    gray = grayscale(image) \n",
    "    mpimg.imsave(\"test_images_gray/\"+img, gray, cmap = cm.gray)\n",
    "    \n",
    "    # Define a kernel size and apply Gaussian smoothing\n",
    "\n",
    "    kernel_arr = [13] #kernel_size = 13 used\n",
    "    for kernel_size in kernel_arr:\n",
    "    \n",
    "        blur_gray = gaussian_blur(gray,kernel_size)\n",
    "        low_threshold = 50\n",
    "        high_threshold = 150\n",
    "        edges = canny(blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "    # This time we are defining a four sided polygon to mask\n",
    "    imshape = image.shape\n",
    "    vertices = np.array([[(imshape[1]-30,imshape[0]),(80, imshape[0]), ((imshape[1]>>1)-60, (imshape[0]>>1)+60), ((imshape[1]>>1)+60, (imshape[0]>>1)+60)]], dtype=np.int32)\n",
    "    masked_edges = region_of_interest(edges, vertices)\n",
    "\n",
    "    # Define the Hough transform parameters\n",
    "    # Make a blank the same size as our image to draw on\n",
    "    rho = 1 # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "    threshold = 15     # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 10 #minimum number of pixels making up a line\n",
    "    max_line_gap = 40    # maximum gap in pixels between connectable line segments\n",
    "    \n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output is an array containing endpoints of detected line segments\n",
    "\n",
    "    \n",
    "    line_img = hough_lines(masked_edges, rho, theta, threshold, \n",
    "                                min_line_length, max_line_gap)\n",
    "    \n",
    "    \n",
    "\n",
    "    line_image = weighted_img(line_img,line_image)\n",
    "    create_dir(\"test_images_output\")\n",
    "    mpimg.imsave(\"test_images_output/\"+img, line_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Video Processing**\n",
    "\n",
    "Video processing was fairly straightforward since by using moviepy.editor to convert video clip to discrete images and then adding lane lines on the images based on the steps above to find lane line in the images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages used for video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "\n",
    "    imshape = image.shape\n",
    "    vertices = np.array([[(imshape[1]-30,imshape[0]),(80, imshape[0]), ((imshape[1]>>1)-60, (imshape[0]>>1)+60), ((imshape[1]>>1)+60, (imshape[0]>>1)+60)]], dtype=np.int32)\n",
    "\n",
    "    #take image input and convert to HLS\n",
    "    img1 = rgb2hls(image)\n",
    "    \n",
    "    img1 = white_yellow_hls(img1)\n",
    "\n",
    "    img1 = grayscale(img1)\n",
    "    img1 = gaussian_blur(img1,kernel_size)\n",
    "    img1 = canny(img1,low_threshold, high_threshold)\n",
    "    img1 = region_of_interest(img1,vertices)\n",
    "    line_img = hough_lines(img1, rho, theta, threshold, min_line_length, max_line_gap)\n",
    "\n",
    "    line_img = weighted_img(line_img,image)\n",
    "    \n",
    "    return line_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing video images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/solidWhiteRight.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidWhiteRight.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████▊| 221/222 [00:06<00:00, 31.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidWhiteRight.mp4 \n",
      "\n",
      "Wall time: 7.84 s\n"
     ]
    }
   ],
   "source": [
    "create_dir(\"test_videos_output\")\n",
    "\n",
    "white_output = 'test_videos_output/solidWhiteRight.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output video results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/solidWhiteRight.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/solidYellowLeft.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidYellowLeft.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████▉| 681/682 [00:23<00:00, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidYellowLeft.mp4 \n",
      "\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "create_dir(\"test_videos_output\")\n",
    "\n",
    "white_output = 'test_videos_output/solidYellowLeft.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(\"test_videos/solidYellowLeft.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/solidYellowLeft.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/challenge.mp4\n",
      "[MoviePy] Writing video test_videos_output/challenge.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 251/251 [00:18<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/challenge.mp4 \n",
      "\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "create_dir(\"test_videos_output\")\n",
    "\n",
    "white_output = 'test_videos_output/challenge.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(\"test_videos/challenge.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/challenge.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a couple of improvements that can be done to further optimize the performance. For example, the lane lines in the. ideo are slightly shaky, by applying IIR filter can help stablize the behavior (note that if heavy filtering is applied then the lane lines may not react fast enough so that is one concern of using IIR filtering). Also, the curvature of the lane lines are not captured in this project.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
